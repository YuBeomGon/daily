0320
1. toxic multi label classification + bert finetunning using pretrained model.
   total accuracy : 90 후반, accuracy : 약 92% 
   
2. naver 영화 리뷰에 대한 pretraining 시작 warm up :30000, total 180000 max_seq_length 64
   data 전처리 어제 저녁 완료
3. animal data에 대한 multi class 작업, 증상코드가 아닌 진단코드로 하자.
4. 진단코드 상위 21개(100개 이상) 에 대한 multi class classification 결과 약 60% accuracy
   label을 id로 바꾼 후 one hot vector로 바꿔줘야 한다. 나중에 np.argmax로 결과값 확인 용이하다.
   
0323
bert/albert pretraining/fine tunning

0324
1. albert pre/fine tunning with naver movie review, (150000~180000 steps for pretrain and fine tune steps) naver train data is used for tfrecord file
   accuracy is 82~83%
2. A detailed example of how to use data generators with Keras 리뷰
3. text clustering( need to make sentence or document to vector using word2vec or bert etc)
   https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert

0327~0330
1. pet data에 대해 albert pre/fine tunning 결과 100% 확인
   pretraining시 corpus를 test, eval set을 제외하고 다시 pre/fine tuninning 진행 중
   data set이 작기 때문에 편향되어 있고 이로 인해 새로운 dataset에 대해 test를 할때 결과가 잘 안나올 수 있다.
   정확도를 낮추더라도 overfit되지 않고 정규화되도록 하는 방안 고민 필요.
   data generation or simulation
   문장 순서 바꾸기, or sentence vector에 적정한 수준의 noise 추가??
   
0331
1. data simulation holding
2. text clustering?
   bert embedding + clustering, 관련 review가 별로 없다. 텍스트 및 목적에 맞게 고민해야 한다.
   animal pet의 경우 label이 단순히 id가 아닌 정보가 담겨 있다. label을 조금 더 문장 수준으로 적고 
   transcription corpus로 pretrain한 후 fine tune까지 조금 해 주고, SE에 대한 bert embedding과 label에 대한 Bert embedding을 
   similarity를 비교하는 식으로 찾는 것은 어떨까?? unsupervised learning을 통해 최대한 label을 맞춰놓기
3. bert 내용 리뷰 및 code reivew, korquad1.0 
  
2. albert를 keras porting하는 법 관련 체크, bert to keras를 참고하고자 하나 error 발생
   https://colab.research.google.com/drive/1UQ-76_hNyU6U2Emkw9qj-Cf1NUt0SPc_#forceEdit=true&sandboxMode=true&scrollTo=kZT3munMvw44
   huggingface/transformer관련 더 확인하자.
   
3. unlabeled data에 대한 clustering 공부해보자. bert를 이용한 doc2vec 후 clustering??
