3/8~3/15
1. object detection관련 기존 과제 backup
(Papsmier, HPV, 동결절편)
2. Linux setting

3/15
to do 
1. Object detection 관련 논문 fast review
   ssd, faster rcnn, mask rcnn, 
2. DETR fast review
3. papsmier 및 DETR 관련 코드 리뷰 및 mixup
4. data 관련 확인, 
   train/test split시 사람별로 나눌 필요 존재?
   모든 object에 bounding box 할 필요 존재?
   data 확인해 보자.
  

done 
1. SSD - hard negative mining - for solving class imbalancing,
         check background bounding box with small loss, (3 vs 1, background vs non background)
         sorting후 loss가 작은 것 3배 sampling해서 학습시 사용한다. 
         
       - small object 잘 못찾는다, 처음 feature를 뽑는 layer에서 아직 abstraction이 충분히 잘 이뤄지지 않음 -> retina net
       - SSD에서는 augmentation으로 극복을 한다.
       - VGG16대신 가벼운 network를 사용시 시간을 더 줄일 수 있다.
       
2. DETR
   set problem, No heuristic like anchor box, NMS.
   Backbone resnet 50, 101, positional embeding 
   Transformer, two FFN for class and bbox
   two loss for matching and backpropagation
   for class imbalance, background class, loss*1/10
   
   Deformabel DETR
   DETR is too long to train 200 epoch, multiple feature impossible(seq len should not be long)
   deformable attention is introduced, -> multiple feature is ok
   
   Locating Object without Bounding Boxes
   not bounding box but point in case of small object
   hausdorf loss
   what is a deformable convolution??
   
   Train/Test set split - random split, No Personal information, but still need to check how to make test data
   regarding Normal class bounding box
      - Bg, high, low(normal in bg)
      - annotate all normal case? -> too much normal case, hard to annotate?
      - annotate several normal case, using loss ft(undersample bg case, by sorting)
      
   code review - bbox and image print 
   
3/16
to do
1. deformable convolution/attention review
   자연어처리 pretraining - vision fine tuning
2. data 관련 확인(병리사님과)
3. DETR 돌려보기, 보규 코드 리뷰

3/19
to do 
1. deformable convolution/attention deep review
2. image 관련 전처리 라이브러리 확인
3. 준환님 코드 리뷰
4. detr training 시작 가능하면

3/22
to do
1. pip vs conda vs venv 등 설정 경로 확인하기
   jupyter 설정, 어떤 python을 사용하는지 등등..
2. detr kuda error 디버깅
3. pap smier article reading
4. cs231n 강의 수강

done
1. cs231n 3강, 4강 수강
   optimization(svm(hinge) loss, cross entropy)
   back propagation
   
2. detr error debugging 관련
   jupyter에서 torch 등 library 설치한 것이 cuda, cudnn등과 호환이 안된 듯
   conda 가상환경에서 설치할 것, 그 후에 jupyter 연동할 것
3 detr loss ft
t1 = torch.tensor([[0.1928, 0.1385, 0.0804, 0.2328, 0.1787, 0.1768],
        [0.1031, 0.0815, 0.0740, 0.1803, 0.4449, 0.1163],
        [0.1860, 0.1658, 0.0975, 0.2000, 0.1930, 0.1577],
        [0.1302, 0.0783, 0.0818, 0.2376, 0.3783, 0.0938]])
t2 = torch.tensor([2], dtype=torch.int8)   
t1[:,t2]
tensors used as indices must be long, byte or bool tensors
label의 data type을 int8에서 uint8로 변경할 것, unsigned로

3/23
1. cs231n 6강
   data preprocessing
   zero mean, unit variance -> relu is good just for first layer
   batch 단위로 mean, 또는 channel 단위로 (RGB) mean을 구한다. 
   test set에서도 동일..
  activation
  weight initialization
  batch normalization
  training시에 mean과 var를 구하고 test시에 이것을 써서 zero mean unit variance normalization을 해준다.
  
3/24
to do
1. 기존 3 class object detecion training
   code 분석
2. New algorithm 이해

done
1. instance vs semantic vs panoptic segmentation 
2. TensorRT, tensorflow example, dataset api 간단 사용
3. pap smear dataset, kaggle code 리뷰
4. 딥러닝을 이용한 논문 리뷰
5. tf.function decorator 1에서 2로 넘어가면서 즉시 실행 모드 지원
   python 함수를 tensorflow graph 바꿔주고, backpropagation가능
   
6. Image pre processing시 batch 단위 mean subtract 또는 channel 단위(RGB)로 해서 값들을 0 근처로 옮겨준다.
   sigmoid의 단점 이루 보완 가능(첫번째 layer만), 
   Loss를 자주 찍어 variation을 자주확인해야 한다. sharp variation means wrong training
   sharp minima can be removed by small data pertubation, we need to find the flat minima
   regularization -> not overfit to training data -> give randomness duing training, batch norm, drop out, remove it during test time
   
7. num_parallel_calls=tf.data.experimental.AUTOTUNE -> cpu data loading multi threading

3/25
to do
1. unet 기반 segmentation fine tuning
   image 관련 전처리 등에 익숙해지자.
   
2. unet 관련, segmentation 관련 강의 들어보자.
done 
1. deeplap v3 세미나 리뷰 
   feature pyramid, encoder /decoder, unet, atrous convolution(dilated con), depthwise con, 1*1 conv
2. Unet backbone 관련 확인 중
   계층적 샘플링(Stratified Sampling), random split시 특정 class가 몰리는 현상을 방지하기 위해 유용할 듯..
   segmentation 관련 유용
   https://tech.socarcorp.kr/data/2020/02/13/car-damage-segmentation-model.html
3. Unet tensorflow 참고 코드 
   https://medium.com/analytics-vidhya/humans-image-segmentation-with-unet-using-tensorflow-keras-fd6cb43b06e5
   https://towardsdatascience.com/nucleus-segmentation-using-u-net-eceb14a9ced4

   

