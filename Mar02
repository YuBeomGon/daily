0306
1. tensorflow estimator관련 예제 확인
2. transformer encoder관련 스터디

0309 
1. albert example 예제 실행, pretraining완료 및 fine tunning까지 됐지만 결과가 잘 안나와서 코드를 좀 더 깊이 봐야 한다.
2. albert 논문 리뷰
  - gpu memory issue로 인한 gradient accumulation, gradient checkpoint, second order optimizaiton등의 방법이 있지만
    albert는 word embedding matrix factorization + layer weight sharing을 통해 parameter의 수를 줄임으로 gpu memory를 절약하였다고,
    SOP, bert에서 다음 문장을 예측할때 오답을 임의로 선택해서 사실상 학습에 도움이 되지 않았다. albert에서는 오답을 정답의 역순으로 대체하였다.
    학습이 더 어려워졌고 학습의 의미가 있다. 
3. bert 논문 리뷰 
   - fine tunning시 pretrained 된 파라미터를 load하고 전체 parameter에 대해 fine tunning을 한다. but how about freezing low layer, low layer
     가 출력과 상대적으로 관련이 적다면 frezzing을 한다면 memory를 save할 수 있다.
     
4. annotated transformer 관련 코드 리뷰 - pytorch는 skip, tensorflow는 추후 보자.
     
5. 파이썬 super keyword : 자식클래스가 부모클래스를 상속할 때, overridig을 한 method에 대해서 부모클래스의 내용도 동작하게 하고 싶을때 super
   를 사용한다. 
   
6. bert code reivew.

0310
do list
1. multiple output classification(label이 여러개 중에서 한개를 택하는 게 아니라 여러개 중에서 2개 3개일수도 있다.)
   how?
2. bert & albert code review
3. overlapping window + fcn
doing
1. gradient accumulation 관련 reading, how to use in keras 관련 medium 체크(pip install runai)
2. gradient를 구할때 메모리 최적화하면서 어떻게?? 관련 생각 정리
3. multi labeled classification 관련 예제 체크
   bert + multi labeled example review
   https://github.com/javaidnabi31/Multi-Label-Text-classification-Using-BERT/blob/master/multi-label-classification-bert.ipynb
   multi labeled classification에서 주의할 점, sampling(batch)을 할때 각 label별로 bias되지 않게 random shuffle해야 한다.
   (https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72)
4. sklearn의 빈도수 기반 classification 테스트
5. multi labeled 문제 푸는 방법 
   1. 여러개의 binary classification문제로 나눠 푼다.
   2. multi label문제를 multi class문제로 변환해서 푼다.
   3. multi label + cross entropy with sigmoid (not softmax)
6. bert 관련 token b는 특정 test case(두문장 연속성 체크)에서만 쓰이고 보통은 token_a만 쓰인다. 문장별로 들어간다. 문단과 문단의 구분은 empty line
   으로 구분한다. 따라서 input csv파일을 만들때 그렇게 만들어져 있어야 한다.
   tokenizer에서 token화 및 interger로 바꾸는 부분만 있는 듯, 그럼 embedding vector로 변환 + positional embedding + sentence embedding 더하는 부분은 
   어디에 있나?
   
   
   0311
 1. bert tokenizer는 tokenizer + id만 담당한다. embedding으로 바꾸는 부분은 modeling하는 부분에 있다. keras의 경우 layer를 쌓을때 맨 처음
    embedding layer를 쌓는데 이때 초기화를 하고 나중에 학습에 의해 학습된다. bert의 경우 modelling하는 부분에서 embeding + position + segment
    등을 더해주는 부분이 있다. 
 2. tx.example Fundamentally, a tf.Example is a {"string": tf.train.Feature} mapping. 
 3. 논문을 보면 bert의 입력은 최대 2문장이다. 그러면 여러줄로 이루어진 문단의 경우 window size of sentence를 2로 해서 들어가는 것일까?
    fine tunning시에 text data를 1문장씩 끊어서 입력하고 각각 나온 cls token을 이용하나?  
    -> answer 논문을 보면 sentence가 문장이 아닌 span of contiguous text라고 표현한다. 문단이라고 봐야 할것 같다.
    
    
 0312
 to do
 1. albert/ bert code reivew
    create pretrain, run_pretrain, io, how to do
 
 
 1. bert tokenizer
    token의 개수 30000로 하지 말고 1000~2000개로 corpus에 specific하게 확 줄이자.
    open source를 돌리거나 수동으로 vocab.txt 수정하자.
 

    
