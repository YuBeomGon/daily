0306
1. tensorflow estimator관련 예제 확인
2. transformer encoder관련 스터디

0309 
1. albert example 예제 실행, pretraining완료 및 fine tunning까지 됐지만 결과가 잘 안나와서 코드를 좀 더 깊이 봐야 한다.
2. albert 논문 리뷰
  - gpu memory issue로 인한 gradient accumulation, gradient checkpoint, second order optimizaiton등의 방법이 있지만
    albert는 word embedding matrix factorization + layer weight sharing을 통해 parameter의 수를 줄임으로 gpu memory를 절약하였다고,
    SOP, bert에서 다음 문장을 예측할때 오답을 임의로 선택해서 사실상 학습에 도움이 되지 않았다. albert에서는 오답을 정답의 역순으로 대체하였다.
    학습이 더 어려워졌고 학습의 의미가 있다. 
3. bert 논문 리뷰 
   - fine tunning시 pretrained 된 파라미터를 load하고 전체 parameter에 대해 fine tunning을 한다. but how about freezing low layer, low layer
     가 출력과 상대적으로 관련이 적다면 frezzing을 한다면 memory를 save할 수 있다.
     
4. annotated transformer 관련 코드 리뷰 - pytorch는 skip, tensorflow는 추후 보자.
     
5. 파이썬 super keyword : 자식클래스가 부모클래스를 상속할 때, overridig을 한 method에 대해서 부모클래스의 내용도 동작하게 하고 싶을때 super
   를 사용한다. 
   
6. bert code reivew.

0310
do list
1. multiple output classification(label이 여러개 중에서 한개를 택하는 게 아니라 여러개 중에서 2개 3개일수도 있다.)
   how?
2. bert & albert code review
3. overlapping window + fcn
doing
1. gradient accumulation 관련 reading, how to use in keras 관련 medium 체크(pip install runai)
2. gradient를 구할때 메모리 최적화하면서 어떻게?? 관련 생각 정리
3. multi labeled classification 관련 예제 체크
   bert + multi labeled example review
   https://github.com/javaidnabi31/Multi-Label-Text-classification-Using-BERT/blob/master/multi-label-classification-bert.ipynb
   multi labeled classification에서 주의할 점, sampling(batch)을 할때 각 label별로 bias되지 않게 random shuffle해야 한다.
   (https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72)
4. sklearn의 빈도수 기반 classification 테스트
5. multi labeled 문제 푸는 방법 
   1. 여러개의 binary classification문제로 나눠 푼다.
   2. multi label문제를 multi class문제로 변환해서 푼다.
   3. multi label + cross entropy with sigmoid (not softmax)
6. bert 관련 token b는 특정 test case(두문장 연속성 체크)에서만 쓰이고 보통은 token_a만 쓰인다. 문장별로 들어간다. 문단과 문단의 구분은 empty line
   으로 구분한다. 따라서 input csv파일을 만들때 그렇게 만들어져 있어야 한다.
   tokenizer에서 token화 및 interger로 바꾸는 부분만 있는 듯, 그럼 embedding vector로 변환 + positional embedding + sentence embedding 더하는 부분은 
   어디에 있나?
   
   
   0311
 1. bert tokenizer는 tokenizer + id만 담당한다. embedding으로 바꾸는 부분은 modeling하는 부분에 있다. keras의 경우 layer를 쌓을때 맨 처음
    embedding layer를 쌓는데 이때 초기화를 하고 나중에 학습에 의해 학습된다. bert의 경우 modelling하는 부분에서 embeding + position + segment
    등을 더해주는 부분이 있다. 
 2. tx.example Fundamentally, a tf.Example is a {"string": tf.train.Feature} mapping. 
 3. 논문을 보면 bert의 입력은 최대 2문장이다. 그러면 여러줄로 이루어진 문단의 경우 window size of sentence를 2로 해서 들어가는 것일까?
    fine tunning시에 text data를 1문장씩 끊어서 입력하고 각각 나온 cls token을 이용하나?  
    -> answer 논문을 보면 sentence가 문장이 아닌 span of contiguous text라고 표현한다. 문단이라고 봐야 할것 같다.
    
    
 0312
 to do
 1. albert/ bert code reivew
    create pretrain, run_pretrain, io, how to do
 
 
 1. bert tokenizer
    token의 개수 30000로 하지 말고 1000~2000개로 corpus에 specific하게 확 줄이자.
    open source를 돌리거나 수동으로 vocab.txt 수정하자.
 2. runconfig -training run시 관련된 configuration을 말하는 듯 tpu/gpu thread등
 
 0316~0317
 1. multi label classification
    상위 4개 label에 대해 multi label classification 결과 약 77%까지(0으로 초기화시 50%)
    상위 8개 label에 대해 multi label classification 결과 약 88%까지(0으로 초기화시 77%)
    mecab과 okt 별 차이 없다.
    bidirectional LSTM 사용
    pandas dataframe 및 정규식 관련 숙련 필요.
 2. bert 관련 sample code 수정
    naver 영화리뷰에서 일부분 data로 동작 확인, run_pretrain_data 수행 시 do_lower_case를 false로 해야 한글에 대해 training data가 제대로 생서왼다.
    naver 전체 data pretraining 중 out of memory 
    
 3. text_a, text_b의 차이
    pretraining단계에서 dataset을 만들때 문단을 자동으로 segment별로 나누고 cls, sep token이 들어가게 된다. 
    fine tuning시에는 목적에 따라 text_b가 있을수도 없을수도 있다. 단순한 classfication이라면 text_b는 없어도 된다.
    즉 segment_id 가 all 0(pretraining에서 0 0 0 0 1 1 1 ...이라는 전제)으로 해도 된다. 
    질문 및 답변이라면 segment id를 각각 0, 1로 맞춰야 한다. 
    
 4. biobert는 english only로 한국어에 대해 쓸수 없다.
 
 5. pretrained된 bert 한국어 모델을 다운받은 후 animal medical classification에 fine tunning해서 동작함 확인
    fine tunning 횟수를 늘리고 내일 확인 예정
    https://ratsgo.github.io/embedding/downloaddata.html
    naver 영화 리뷰를 대상으로 vocab.txt 생성부터 pretraining까지 완료, fine tunning 진행 중, 내일 결과 확인해 보자.
    
 6. multi label classification의 경우 대부분 0이고 이로 인해 학습이 안 이루어질 수 있다. keras에 weight를 주는 방법이 있다. 
    또는 customized loss function을 사용해 보자.  아래 링크 참조
    https://stackoverflow.com/questions/50686217/keras-how-is-accuracy-calculated-for-multi-label-classification
    https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu/313922#313922

 0319
 1. google colab이 연결이 끊겨서 테스트 불가능하다.
