  0207
1. Tensorflow 음성인식 예제 및 Lingvo, openseqtoseq 및 tensorpack 음성인식 관련 체크,
   Tensorflow 음성인식 및 tensorpack 음성인식 예제 설치 에러 발생, 필요시 추가 확인할 것
   괜찮은 음성인식 예제가 있어 코드 분석
   https://github.com/philipperemy/tensorflow-ctc-speech-recognition
   RNN에서 input이 variable seq이다. wav file을 max로 맞추고 나머지는 padding bit(0)로 채우고 transcription에서는 blank가 된다. 
   padding을 채움으로써 RNN의 seq_len을 동일하게 맞출 수 있다.
   
   FFT 란 : 주파수분석을 할때 주파수 bin의 개념을 도입 주파수를 블럭 단위로 양자화한다. 주파수 변환을 더 빠르게 할수 있다. 
   filter bank의 개념과 유사.
 2.  https://github.com/philipperemy/tensorflow-ctc-speech-recognition 예제 training 시작, 코드 분석 필요.
   training이 잘 안되서 중지함
 
 3. audio to deep learning 관련 학습해보자. 
 https://www.endpoint.com/blog/2019/01/08/speech-recognition-with-tensorflow
   중간에 notfound module error 발생

0210
1. philipperemy training 중 gpu memory issue 발생(deep speech와 같이 돌려서), 주말 동안 어느 정도 정확해 진듯 하다.
2. kamil의 경우 docker install 후 make run에서 CRIT Supervisor running as root 와 같은 에러 발생, 도커 uninstall후 재 테스트
3. AI hub data로 테스트시 약 22%의 wer 발생, 음절단위임을 생각하면 wer은 조금 더 높아질 듯, 다만 train/test set을 사람별로 나눈게 아님,
   zeroth data로 test시 약 음절 기준 약 68%나온다. 
4. evaluation set을 zeroth로 잡고 over fittingdmf 잡아보자.

0211
1. MFCC가 화자 관계없이 순수하게 음성의 feature를 뽑는 것이라면 전혀 새로운 화자의 음성도 인식할 수 있어야 한다.
2. AI hub data로 training, zeroth로 dev를 잡고 training해도 잘 안됨.
   관련하여 zeroth domain에서 테스트 할것, ngram order를 6가 아닌 그 이상으로 주자, kenlm 및 ds decoder 확인 및 수정 필요
3. FFT에서 주파수 영역과 주파수 크기에 대해 양자화가 이루어진다면 더 빨라질 수 있는 방법 없을까?
4. 음성을 2d dimension으로 보고 2d convolution에 대해 생각해 봄, 시간에 대한 context를 뽑는 것은 의미가 있지만 freq domain으로 context를 뽑는 
   것은 사실 의미가 없다. -> 1d convolution이 필요하다. spectogram등을 뽑을 때 normalization 등 다양한 방법 존재.
   발성 및 청각에 대한 배경지식 공부, domain에 대한 지식이 필요하나 전문적인 용어로 인해 진입장벽이 있다. 
   speech recognition도 인간의 청각시스템을 이해하고 그것을 dnn으로 모델링하는 것이 필요해 보인다. -> 관련 스터디 필요
5. kenlm compile 시 install Eigen3 in your home directory 에러는 아무 상관 없는 듯, arpa와 binary 잘 만들어진다. 

todo
1. 언어모델 12~15 gram으로 바꿔서 wer 볼것, shared lib 및 파이썬 래핑 확인 필요
2. mfcc feature 개수를 현재 13에서 더 낮추어서 테스트 해 볼것, personal specific한 정보를 최대한 줄여야 한다. 

0212
1. dsdecoder 및 kenlm 에서 MAX_ORDER를 15로 변경후 deep speech decoding이 잘 되는 것 확인하였다. 
   - DeepSpeech/native_client내에 kenlm을 삭제하고 git에서 새로 받은 후 max_order 15로 해서 recompile
   - DeepSpeech/native_client/ctcdecoder내 build_common.py에서 max_order 15로 변경후 ds decoder 재 컴파일
   - dist내 생성된 whl파일을 이용 pip3 install로 ds_decoder 재설치하면 된다.
2. weight관련 어느 정도 안정화 된 상태에서 gradient가 완만한 w는 많이 급격한 w는 조금씩 변경하도록 weight를 변경해볼 필요 있어 보임(장기 연구과제)
3. Tensorflow 관련 강의 시청
4. DeepSpeech code review and search - inference 하는 부분, data를 320ms(16 time step)씩 잘라서 model에 돌려서 output을 얻는다 .이것을 
   wave file이 끝날때까지 돌리고 output tensor에 대해 ctc decoder를 돌려 inferencing한다. 
   
     


   
