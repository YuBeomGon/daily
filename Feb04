0221
1. data iteration 부분 확인하고 tf.api.dataset쓰는 것도 한번 고려해 보자.
2. generator 및 zip 관련수정 후 batch iteration 성공, text 분류 성공하였다. 
3. Fasttext 관련 서치
   fasttext training, gensim을 이용 word2vec으로 model load시 out of vocab 에러 발생가능
   직접 fasttext를 로드해야 없는 단어도 벡터화 가능
   문장은 단순히 각 단어 벡터의 평균을 내도 좋다.
   주말동안 word2vec에 대해 학습해 보자.

0224
1. sent2vec을 이용 문장 벡터화 후에 신경망 모델 훈련, 약 75% accuracy
2. sentvec model training시 corpus를 형태소 분석 후 명사, 형용사 부사 동사만 취해서 corpus로 저장후 이것에 대한 fasttext training실시.
   이 sen2vec을 이용 신경망 training시 약 70% 정도 나온다.
3. gensim, sklearn 간단리뷰, sklearn을 통해 count 기반의 bags of word, tf idf표현할 수 있다. 하지만 단어의 개수 등 지정하는 법 아직 모르겠음
   library가 아닌 구현된 코드 이용하는 것도 나쁘지 않다. tf idf는 코드 구해야 됨.
4. bert, text mining의 끝판왕, 복잡하고 시간이 오래 걸릴 듯. pretraining을 많이 해야 한다. 

5. 하고자 하는 모델에 맞게 fine tunning된 모델을 사용해 보자.(bert는 차선)
   1. cnn model에서 input lookup table saving, -> input lookup table은 분류를 하기 위해 가장 적절한 값으로 저장.
      but labeled data가 8000개가 안된다. 
      전체 data를 대상으로 fast text등을 학습시키는 것이 더 좋다.
   2. overlapping window + FCN or convolution ?
      convolution을 하면 중첩해서 쌓았을때 context window가 넓어지는 장점 존재.
      FCN은 window size가 overlapping window로 고정됨, 그 외에는 동일하다.
   3. 영화에서는  9-10 : good, 1~4 : bad로 output class를 2로 두었다. 그렇게 두지 말고 output class도 0~1까지 label에 따라 확률로 둬도 될까?
      그럴 경우에도 cross entropy 계산 가능할까?
      안된다면 평점에 맞게 data를 비례적으로 구성..
      
0225
1. perplexity란 언어모델 관련 측정 도구, training set으로 정확도 측정한다.

0227
1. keras를 이용한 영화리뷰 분류, word embeding, tokenize, dnn,cnn,lstm을 이용한 분류 예제 실행
   https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/
2. bert 예제 실행
   bert embedding 및 bert classify training 모두 gpu outofmemory issue로 training 불가능하다.
   google colab에서 테스트 필요
   
0228
1. bert에 대한 이론적 배경 쌓음
   transformer의 encoder 부분만 취함
   rnn의 구조를 버리고 self attention을 취함, 각 단어간의 지역적인, 위치 정보를 취하기 위해 positional embedding의 개념 도입
   embeding이 고정되지 않고 parameter를 줘서 학습이 된다는 개념이 신선하다. parameter는 단어의 위치 또는 dimension 내에 위치로서 
   같은 단어일지라도 위치에 따라 다른 임베딩값을 같게 된다.(기본 임베딩 + 위치 임베딩)
   self attention - word2vec, fasttext, glove등을 통해 label이 없는 text data에 대해 학습을 해서 단어를 embedding시킬 수 있게 되었다.
   그러나 이것 말고 더 학습할 수 있는 것이 있다면 최대한 학습하는 것이 좋을 것이다. 텍스트에서 최대한 context를 뽑아내자.
   self attention은 그러한 의도에 의해 나타났다. query와 key값이 모두 입력 문장내의 단어들이다. 단어들 각각의 유사도를 내적 또는 비슷한 연산
   을 통하여 구한다. attention을 목적에 따라 병렬로 여러번 수행한다. multi head
   이렇게 함으로써 단어는 여러단어의 linear combination으로 표현되고 전체 문맥을 고려해서 embeding된다. 같은 단어일지라도 문맥에 따라 값이 달라질 수 있다.
   입력을 위치 정보를 고려한 positional encoding으로 고정해 놓고 그 위에 multi head attention + fcn을 여러개 쌓아서 최종적인 출력은 각
   training data의 문맥을 가장 잘 표현하는 word vector가 될 것이다.
   
   training은 어떻게 시켜야 할까? open gpt의 경우 언어모델로 훈련을 시킨다. i have a * 다음에 어떤 단어가 올지. 따라서 단방향 모델이 될수밖에 없다.
   Bert는 양뱡향을 사용하여 훈련하다, but How??
   전체 corpus에서 10~15%의 token을 masking하고 그것을 양방향 모델을 훈련시켜 예측한다. 또한 예측 뿐만 아니라 masking된 단어에 classify까지 하며
   training한다, 이로써 open gpt에 비해 적은 training 학습을 보충한다. 

      
      

   
