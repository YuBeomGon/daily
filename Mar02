0306
1. tensorflow estimator관련 예제 확인
2. transformer encoder관련 스터디

0309 
1. albert example 예제 실행, pretraining완료 및 fine tunning까지 됐지만 결과가 잘 안나와서 코드를 좀 더 깊이 봐야 한다.
2. albert 논문 리뷰
  - gpu memory issue로 인한 gradient accumulation, gradient checkpoint, second order optimizaiton등의 방법이 있지만
    albert는 word embedding matrix factorization + layer weight sharing을 통해 parameter의 수를 줄임으로 gpu memory를 절약하였다고,
    SOP, bert에서 다음 문장을 예측할때 오답을 임의로 선택해서 사실상 학습에 도움이 되지 않았다. albert에서는 오답을 정답의 역순으로 대체하였다.
    학습이 더 어려워졌고 학습의 의미가 있다. 
3. bert 논문 리뷰 
   - fine tunning시 pretrained 된 파라미터를 load하고 전체 parameter에 대해 fine tunning을 한다. but how about freezing low layer, low layer
     가 출력과 상대적으로 관련이 적다면 frezzing을 한다면 memory를 save할 수 있다.
     
4. annotated transformer 관련 코드 리뷰 - pytorch는 skip, tensorflow는 추후 보자.
     
5. 파이썬 super keyword : 자식클래스가 부모클래스를 상속할 때, overridig을 한 method에 대해서 부모클래스의 내용도 동작하게 하고 싶을때 super
   를 사용한다. 
   
6. bert code reivew.

0310
do list
1. multiple output classification(label이 여러개 중에서 한개를 택하는 게 아니라 여러개 중에서 2개 3개일수도 있다.)
   how?
2. bert & albert code review
3. overlapping window + fcn

1. gradient accumulation 관련 reading, how to use in keras 관련 medium 체크(pip install runai)
2. gradient를 구할때 메모리 최적화하면서 어떻게?? 관련 생각 정리
3. multi labeled classification 관련 예제 체크
   bert + multi labeled example review
   https://github.com/javaidnabi31/Multi-Label-Text-classification-Using-BERT/blob/master/multi-label-classification-bert.ipynb
   multi labeled classification에서 주의할 점, sampling(batch)을 할때 각 label별로 bias되지 않게 random shuffle해야 한다.
   (https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72)
4. sklearn의 빈도수 기반 classification 테스트
5. multi labeled 문제 푸는 방법 
   1. 여러개의 binary classification문제로 나눠 푼다.
   2. multi label문제를 multi class문제로 변환해서 푼다.
   3. multi label + cross entropy with sigmoid (not softmax)
6. bert 관련 token b는 특정 test case(두문장 연속성 체크)에서만 쓰이고 보통은 token_a만 쓰인다. 문장별로 들어간다. 문단과 문단의 구분은 empty line
   으로 구분한다. 따라서 input csv파일을 만들때 그렇게 만들어져 있어야 한다.
   tokenizer에서 token화 및 interger로 바꾸는 부분만 있는 듯, 그럼 embedding vector로 변환 + positional embedding + sentence embedding 더하는 부분은 
   어디에 있나?
   

    
